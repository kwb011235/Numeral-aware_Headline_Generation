{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of the Evaluation\n",
    "The goal is to generate numeral-aware headlines for news articles. The evaluation focuses on two main aspects:\n",
    "1. How well the headlines are summarised and capture the key numerical information from the article\n",
    "2. The correctness of this numerical information (numerical reasoning capability)\n",
    "\n",
    "## The Evaluation Process\n",
    "The notebook sets up an evaluation pipeline for three different model configurations:\n",
    "1. **Base Model**: Original Llama 3.1-8B-Instruct model without fine-tuning\n",
    "2. **Fine-Tuned Model**: Llama 3.1 model fine-tuned on the NumHG dataset\n",
    "3. **Chain-of-Thought Fine-Tuned Model**: The fine-tuned model with additional prompting that guides the model through a reasoning process (chain of thought)\n",
    "\n",
    "## Evaluation Metrics\n",
    "The NumHG evaluation script (numhg_eval.py) implements several metrics mentioned in the project PDF:\n",
    "* **Accuracy**: Assessing the correctness of numerical information\n",
    "* **ROUGE**: Evaluating text overlap with reference headlines\n",
    "* **BERTScore**: Measuring semantic similarity using contextual embeddings\n",
    "* **MoverScore**: Computing semantic similarity with earth mover distance\n",
    "\n",
    "## Dataset\n",
    "The evaluation uses the NumHG dataset (Numerical Headline Generation), which contains:\n",
    "* Target headlines (ground truth)\n",
    "* News articles\n",
    "* Ground truth numerical values\n",
    "* Number types information\n",
    "\n",
    "## Flow of the Evaluation Code\n",
    "1. The code first sets up the environment by cloning repositories and installing dependencies\n",
    "2. It prepares the necessary evaluation metrics (MoverScore, ROUGE, etc.)\n",
    "3. It then runs the evaluation script three times, once for each model variant, comparing their generated headlines against the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5128,
     "status": "ok",
     "timestamp": 1733358902079,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "ArHpBm4ED7nC",
    "outputId": "9f7b3080-eb7d-4333-bc66-6187aa929fef"
   },
   "outputs": [],
   "source": [
    "# Clone the NumHG repository (Numerical Headline Generation)\n",
    "# This repository contains the dataset and evaluation code for the task\n",
    "!git clone https://github.com/ArrowHuang/NumHG.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 22460,
     "status": "ok",
     "timestamp": 1733358925900,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "rUXaloN-EQCL",
    "outputId": "fbdd1a8f-ddbb-4066-9143-359c2cac6b0b"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies for the evaluation\n",
    "# The repository requirements include all necessary packages for evaluation metrics\n",
    "!pip install -r NumHG/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1898,
     "status": "ok",
     "timestamp": 1733358964009,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "kwu2zLsDEjge",
    "outputId": "851fe656-7a21-4920-c377-e65e35e0a858"
   },
   "outputs": [],
   "source": [
    "# Clone the MoverScore repository\n",
    "# MoverScore is an evaluation metric that measures semantic similarity between texts\n",
    "# It's one of the evaluation metrics mentioned in the project outline\n",
    "!git clone https://github.com/AIPHES/emnlp19-moverscore.git\n",
    "%cd emnlp19-moverscore/\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55897,
     "status": "ok",
     "timestamp": 1733359028595,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "PvLIWgj_EtwH",
    "outputId": "58842db8-dd96-40f1-dd82-bf0a999af8d6"
   },
   "outputs": [],
   "source": [
    "# Update TensorFlow and Keras to the latest versions for compatibility\n",
    "!pip install --upgrade tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9985,
     "status": "ok",
     "timestamp": 1733359074502,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "P1QQKm-fFZCE",
    "outputId": "5cd61fc7-fe7b-4a6c-ec19-6d723ceb81ed"
   },
   "outputs": [],
   "source": [
    "# Install the Transformers library for working with pre-trained models\n",
    "# This is needed for several evaluation metrics including BERTScore and MoverScore\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6683,
     "status": "ok",
     "timestamp": 1733359100537,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "V2Ag9FtlFdn8",
    "outputId": "1e4733e1-e105-417c-be82-bb44a0f98e05"
   },
   "outputs": [],
   "source": [
    "# Install pytorch-pretrained-bert for MoverScore functionality\n",
    "\n",
    "# import os\n",
    "# os.environ['MOVERSCORE_MODEL'] = \"albert-base-v2\"\n",
    "# from moverscore_v2 import word_mover_score, get_idf_dict\n",
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1733359104070,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "OzpwCI-YFkxK",
    "outputId": "f91f2303-1aed-4920-a4f4-8e4fd0cbebf2"
   },
   "outputs": [],
   "source": [
    "# Navigate back to the NumHG directory for evaluation\n",
    "%cd ../NumHG/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3634,
     "status": "ok",
     "timestamp": 1733359108734,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "-SCHdiUuF4da",
    "outputId": "00788153-022b-4510-e4ee-494e9a10dc11"
   },
   "outputs": [],
   "source": [
    "# Download NLTK punktab tokenizer for text tokenization\n",
    "# This is needed for proper text preprocessing during evaluation\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94962,
     "status": "ok",
     "timestamp": 1733359269303,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "RHzg7rNIqOgF",
    "outputId": "f54bf174-c626-4351-dacc-7b7e88d894e3"
   },
   "outputs": [],
   "source": [
    "# Run the evaluation script on the base model predictions\n",
    "# This evaluates the original Llama 3.1 model without fine-tuning\n",
    "# Parameters:\n",
    "# - tgt_path: Ground truth headlines\n",
    "# - pre_path: Model predictions from base model\n",
    "# - num_gt_path: Ground truth for numerical values\n",
    "# - num_type_path: Types of numbers in each headline\n",
    "!python numhg_eval.py \\\n",
    "--tgt_path=Dataset/fold-1/target.txt \\\n",
    "--pre_path=../BASEpreds-head.txt \\\n",
    "--num_gt_path=Dataset/fold-1/number_gt.txt \\\n",
    "--num_type_path=Dataset/fold-1/number_type.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61530,
     "status": "ok",
     "timestamp": 1733359335807,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "Twn2Kizc1Gpq",
    "outputId": "1e61e8bc-780a-4fba-a901-4c86108cc129"
   },
   "outputs": [],
   "source": [
    "# Run the evaluation script on the fine-tuned model predictions\n",
    "# This evaluates the Llama 3.1 model after basic fine-tuning\n",
    "# Using the same ground truth paths as above but different predictions\n",
    "!python numhg_eval.py \\\n",
    "--tgt_path=Dataset/fold-1/target.txt \\\n",
    "--pre_path=../FTpreds-head.txt \\\n",
    "--num_gt_path=Dataset/fold-1/number_gt.txt \\\n",
    "--num_type_path=Dataset/fold-1/number_type.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61771,
     "status": "ok",
     "timestamp": 1733359429919,
     "user": {
      "displayName": "Sean O'Sullivan",
      "userId": "15237573995124798234"
     },
     "user_tz": 360
    },
    "id": "d-ZemJgfGQhG",
    "outputId": "219c221f-e37d-4b75-ef57-089e771b7d60"
   },
   "outputs": [],
   "source": [
    "# Run the evaluation script on the chain-of-thought fine-tuned model\n",
    "# This evaluates the Llama 3.1 model that was fine-tuned with chain-of-thought prompting\n",
    "# Which provides more detailed instructions during generation\n",
    "!python numhg_eval.py \\\n",
    "--tgt_path=Dataset/fold-1/target.txt \\\n",
    "--pre_path=../FTCOTpreds-head.txt \\\n",
    "--num_gt_path=Dataset/fold-1/number_gt.txt \\\n",
    "--num_type_path=Dataset/fold-1/number_type.txt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNK9UEbIjprTTvBBnP3inEn",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
